{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWhQdWO_4qqe"
   },
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "  <h1></h1>\n",
    "  <h1>Stylized Retrieval-Augmented Generation</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi69xdNwKZPv"
   },
   "source": [
    "In this notebook, I will build and implement a Retrieval-Augmented Generation (RAG) pipeline tailored for a text style transfer application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bss2qVkJKkZr"
   },
   "source": [
    "### Table of Contents\n",
    "- [1. Access to Hugging Face](#1-access-to-hugging-face)\n",
    "- [2. Packages](#2-packages)\n",
    "- [3. Problem Statement](#3-problem-statement)\n",
    "- [4. Fetch and Parse](#4-fetch-and-parse)\n",
    "- [5. Calculate Word Stats](#5-calculate-word-stats)\n",
    "- [6. Set Up LLM](#6-set-up-llm)\n",
    "- [7. BM25 Retriever](#7-bm25-retriever)\n",
    "- [8. Build Chroma](#8-build-chroma)\n",
    "- [9. Ensemble Retriever](#9-ensemble-retriever)\n",
    "- [10. Format Documents](#10-format-documents)\n",
    "- [11. RAG Chain](#11-rag-chain)\n",
    "- [12. Final Response](#12-final-response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZWrcJ4LKneE"
   },
   "source": [
    "# 1. Access to Hugging face\n",
    "Execute the following cell to connect to your Hugging Face account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pe3-F-kiKYiD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Huggingfacehub API token: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Prompt user for Hugging Face API token if not already set\n",
    "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Huggingfacehub API token: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N72UZSMKqSa"
   },
   "source": [
    "# 2. Packages\n",
    "Execute the following code cells for installing the packages needed for creating your Stylized RAG.\n",
    "\n",
    "note: If there are package conflics you can use pip-tools to automatically find and install the compatible versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WLei89zCKsEj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing requirements for protobuf: [Errno 2] No such file or directory: '/Users/vishalbanwari/miniconda3/lib/python3.10/site-packages/protobuf-4.25.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for protobuf: [Errno 2] No such file or directory: '/Users/vishalbanwari/miniconda3/lib/python3.10/site-packages/protobuf-4.25.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for protobuf: [Errno 2] No such file or directory: '/Users/vishalbanwari/miniconda3/lib/python3.10/site-packages/protobuf-4.25.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: No metadata found in /Users/vishalbanwari/miniconda3/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Cannot uninstall protobuf 4.25.3, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps protobuf==4.25.3'.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for protobuf: [Errno 2] No such file or directory: '/Users/vishalbanwari/miniconda3/lib/python3.10/site-packages/protobuf-4.25.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for protobuf: [Errno 2] No such file or directory: '/Users/vishalbanwari/miniconda3/lib/python3.10/site-packages/protobuf-4.25.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for protobuf: [Errno 2] No such file or directory: '/Users/vishalbanwari/miniconda3/lib/python3.10/site-packages/protobuf-4.25.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for protobuf: [Errno 2] No such file or directory: '/Users/vishalbanwari/miniconda3/lib/python3.10/site-packages/protobuf-4.25.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for protobuf: [Errno 2] No such file or directory: '/Users/vishalbanwari/miniconda3/lib/python3.10/site-packages/protobuf-4.25.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q langchain\n",
    "!pip install -q langchain-community\n",
    "!pip install -q langchain-chroma\n",
    "!pip install -q langchain-huggingface\n",
    "!pip install -q bs4\n",
    "!pip install -q rank_bm25\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2K3p5lXKuSy"
   },
   "source": [
    "# 3. Problem Statement\n",
    "I will implement **Text Style Transfer**, a technique that modifies text style while preserving its content. I will build an **ensemble retriever** combining **BM25** for keyword-based retrieval and **Chroma** for semantic search to retrieve relevant documents, which will be used as input for the style transfer process. This project integrates classical retrieval methods with modern neural embeddings for practical NLP applications.\n",
    "\n",
    "**what is text style transfer?**\n",
    "\n",
    "**Text Style Transfer** is a natural language processing (NLP) technique that modifies the style of a given text while preserving its original content. It allows for the transformation of linguistic expressions to convey different tones, emotions, or writing styles without altering the underlying meaning. For example, it can rephrase formal text into a casual tone, adapt neutral statements into an emotional tone, or convert modern language into a Shakespearean style. This technique has applications in personalized communication, creative writing, sentiment adjustment, and even domain adaptation, making it a powerful tool for generating diverse textual outputs tailored to specific needs.\n",
    "\n",
    "### Example of Text Style Transfer:\n",
    "\n",
    "#### **Input (Neutral Tone):**\n",
    "\"I am excited about the opportunity to work on this project.\"\n",
    "\n",
    "#### **Output (Formal Tone):**\n",
    "\"I am genuinely enthusiastic about the prospect of contributing to this project.\"\n",
    "\n",
    "#### **Output (Casual Tone):**\n",
    "\"I'm super pumped to get started on this project!\"\n",
    "\n",
    "#### **Output (Shakespearean Style):**\n",
    "\"Verily, I am thrilled by the chance to partake in this noble endeavor.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQc6rEV4MHfY"
   },
   "source": [
    "# 4. Fetch and Parse\n",
    "\n",
    "*    Fetching and parsing web content: Write a function that fetches the HTML content of a webpage and processes it to extract clean, readable text.\n",
    "*    Splitting text into smaller chunks: Implement a function to split the text into overlapping chunks, ensuring that each chunk is manageable for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0kxayMljLPhu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def fetch_and_parse(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the webpage content at `url` and return a cleaned string of text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Fetch webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        # Step 2: Parse HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Step 3: Remove unwanted elements (scripts, styles, etc.)\n",
    "        for element in soup(['script', 'style', 'head', 'header', 'footer', 'nav']):\n",
    "            element.decompose()\n",
    "            \n",
    "        # Step 4: Extract and clean text content\n",
    "        text = soup.get_text()\n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def split_text_into_documents(text: str, chunk_size: int = 1000, overlap: int = 100):\n",
    "    \"\"\"\n",
    "    Split a long text into overlapping chunks and return them as Documents.\n",
    "    \"\"\"\n",
    "    # Initialize empty list for documents\n",
    "    docs = []\n",
    "    \n",
    "    # If text is shorter than chunk_size, return it as a single document\n",
    "    if len(text) <= chunk_size:\n",
    "        return [Document(page_content=text)]\n",
    "    \n",
    "    # Split text into overlapping chunks\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        # Get chunk of text\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # If we're not at the last chunk, try to find a good breaking point\n",
    "        if end < len(text):\n",
    "            # Try to find the last period or space before the end\n",
    "            last_period = chunk.rfind('.')\n",
    "            last_space = chunk.rfind(' ')\n",
    "            break_point = max(last_period, last_space)\n",
    "            \n",
    "            if break_point != -1:\n",
    "                chunk = chunk[:break_point + 1]\n",
    "                end = start + break_point + 1\n",
    "        \n",
    "        # Create Document object and add to list\n",
    "        docs.append(Document(page_content=chunk))\n",
    "        \n",
    "        # Move start position, accounting for overlap\n",
    "        start = end - overlap\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmZQ8HhGPugo"
   },
   "source": [
    "# 5. Calculate Word Stats\n",
    "\n",
    "1. Calculate the total number of words and characters across all documents.\n",
    "2. Compute the average number of words and characters per document.\n",
    "3. Print the average statistics in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BkmqrEeYM-8z"
   },
   "outputs": [],
   "source": [
    "def calculate_word_stats(texts):\n",
    "    \"\"\"\n",
    "    Calculate and display average word and character statistics for a list of documents.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (list): A list of Document objects, where each Document contains a `page_content` attribute.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints the average word and character counts per document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Initialize variables to keep track of total words and total characters.\n",
    "    total_words, total_characters = 0, 0\n",
    "    \n",
    "    #Handle empty input\n",
    "    if not texts:\n",
    "        print(\"No documents provided\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    # Step 2: Iterate through each document in the `texts` list.\n",
    "    for doc in texts:\n",
    "        content = doc.page_content\n",
    "        \n",
    "        words = content.split()\n",
    "        total_words  += len(words)\n",
    "        \n",
    "        total_characters += len(content)\n",
    "        # Hint: `doc.page_content` contains the text of the document.\n",
    "\n",
    "    # Step 3: Calculate the average words and characters per document.\n",
    "    # - Avoid division by zero by checking if the `texts` list is not empty.\n",
    "    num_docs = len(texts)\n",
    "    avg_words = total_words / num_docs\n",
    "    avg_characters = total_characters / num_docs\n",
    "\n",
    "    # Step 4: Print the calculated averages in a readable format.\n",
    "    # Example: \"Average words per document: 123.45\"\n",
    "    print(f\"Average words per document: {avg_words}\")\n",
    "    print(f\"Average characters per document: {avg_characters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EpNcNKHdh_ar"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average words per document: 7.75\n",
      "Average characters per document: 44.5\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to test your calculate_word_stats function.\n",
    "# Create sample Document objects with text content for testing your code above.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"This is the first test document.\"),\n",
    "    Document(page_content=\"Here is another example document for testing.\"),\n",
    "    Document(page_content=\"Short text.\"),\n",
    "    Document(page_content=\"This document has more content. It's longer and has more words in it for testing purposes.\"),\n",
    "]\n",
    "\n",
    "# Call the function with the sample documents to calculate word statistics.\n",
    "calculate_word_stats(sample_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIIdfz7tlPdG"
   },
   "source": [
    "# 6. Set Up LLM\n",
    "This function will:\n",
    "\n",
    "1. Initialize and connect to a pre-trained model available on Hugging Face.\n",
    "2. Allow customization of parameters like the model repository ID and generation temperature.\n",
    "3. Return the configured LLM object, which will be used later for text generation tasks in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GnwtA4xolKWt"
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "def setup_llm(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"\n",
    "    Set up and return a Hugging Face LLM using the specified model repository ID and generation parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - repo_id (str): The repository ID of the Hugging Face model to use (default: \"mistralai/Mistral-7B-Instruct-v0.3\").\n",
    "    - temperature (float): The generation temperature to control creativity in outputs (default: 1.0).\n",
    "\n",
    "    Returns:\n",
    "    - HuggingFaceEndpoint: A configured LLM object ready for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Import the HuggingFaceEndpoint class.\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id = repo_id,\n",
    "        temperature = 0.7)\n",
    "    \n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdEyYy86oxn9"
   },
   "source": [
    "# 7. BM25 Retriever\n",
    "\n",
    "1. Initialize the BM25 retriever with a set of documents.\n",
    "2. Implement a method to retrieve the top k most relevant documents for a given query.\n",
    "3. Use efficient tokenization and scoring to ensure accurate and fast results.\n",
    "This component will enable the pipeline to fetch relevant information from a corpus, which is then passed to the LLM for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "kp7Th24kmKLm"
   },
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class BM25Retriever:\n",
    "    \"\"\"\n",
    "    A class to implement BM25-based document retrieval.\n",
    "    \"\"\"\n",
    "    def __init__(self, documents):\n",
    "        \"\"\"\n",
    "        Initialize the BM25 retriever with the given documents.\n",
    "        \"\"\"\n",
    "        # Store the original documents\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Extract text content from documents and create corpus\n",
    "        self.corpus = [doc.page_content for doc in documents]\n",
    "        \n",
    "        # Tokenize the corpus - split each document into words\n",
    "        self.tokenized_corpus = [doc.split() for doc in self.corpus]\n",
    "        \n",
    "        # Initialize BM25 with tokenized corpus\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "\n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve the top k most relevant documents for a given query.\n",
    "        \"\"\"\n",
    "        # Tokenize the query\n",
    "        tokenized_query = query.split()\n",
    "        \n",
    "        # Get document scores using BM25\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get indices of top k scoring documents\n",
    "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "        \n",
    "        # Return the original documents in order of relevance\n",
    "        return [self.corpus[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skPWlylvpWgB"
   },
   "source": [
    "Execute the following code to test the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "qrEkVO9UpUtk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Relevant Documents:\n",
      "1. Machine learning is a method of data analysis that automates analytical model building.\n",
      "2. Deep learning is a subset of machine learning that uses neural networks with three or more layers.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create sample Document objects.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning is a method of data analysis that automates analytical model building.\"),\n",
    "    Document(page_content=\"Deep learning is a subset of machine learning that uses neural networks with three or more layers.\"),\n",
    "    Document(page_content=\"Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing is a field of AI focused on the interaction between computers and human language.\"),\n",
    "]\n",
    "\n",
    "# Initialize the retriever with the sample documents.\n",
    "retriever = BM25Retriever(sample_docs)\n",
    "\n",
    "# Test the retriever with a query.\n",
    "query = \"What is machine learning?\"\n",
    "top_docs = retriever.retrieve(query, k=2)\n",
    "\n",
    "# Print the results.\n",
    "print(\"Top Relevant Documents:\")\n",
    "for idx, doc in enumerate(top_docs, 1):\n",
    "    print(f\"{idx}. {doc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmdT52BtqwfQ"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "Top Relevant Documents:\n",
    "1. Machine learning is a method of data analysis that automates analytical model building.\n",
    "2. Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZhBH_Aqs2of"
   },
   "source": [
    "# 8. Build Chroma\n",
    "\n",
    "1. Initializing a vector store (Chroma) with Hugging Face embeddings.\n",
    "2. Adding a list of documents to the vector store.\n",
    "3. Returning the vector store for later use in the retrieval and generation pipeline.\n",
    "\n",
    "This function sets up the semantic retrieval system, allowing for more meaningful and context-aware results than keyword-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YiAtoavjpgPt"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "def build_chroma(documents: list[Document]) -> Chroma:\n",
    "    \"\"\"\n",
    "    Build a Chroma vector store using Hugging Face embeddings.\n",
    "    \"\"\"\n",
    "    # Initialize Hugging Face embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "    \n",
    "    # Initialize Chroma vector store\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"EngGenAI\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    # Add documents to the vector store\n",
    "    # This will automatically compute embeddings for all documents\n",
    "    vector_store.add_documents(documents)\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivy_VQFItWTr"
   },
   "source": [
    "Execute the following code to test the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-0Vb-uDutN3-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/fz5775953fsb5mn1tqdsv6z00000gn/T/ipykernel_99222/1536423321.py:15: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store built successfully!\n",
      "<langchain_community.vectorstores.chroma.Chroma object at 0x35981f910>\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create sample Document objects.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning is a method of data analysis that automates analytical model building.\"),\n",
    "    Document(page_content=\"Deep learning is a subset of machine learning that uses neural networks with three or more layers.\"),\n",
    "    Document(page_content=\"Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing is a field of AI focused on the interaction between computers and human language.\"),\n",
    "]\n",
    "\n",
    "# Call the function to build the Chroma vector store.\n",
    "vector_store = build_chroma(sample_docs)\n",
    "\n",
    "# Test retrieval (optional, if supported).\n",
    "print(\"Vector store built successfully!\")\n",
    "print(vector_store)  # Print the vector store object to verify.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TgIqUZGttLE"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "Vector store built successfully!\n",
    "\n",
    "<langchain.vectorstores.Chroma object at 0x7f8c1a4b3f10>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgxRBNaxv-ro"
   },
   "source": [
    "# 9. Ensemble Retriever\n",
    "\n",
    "1. Retrieve documents from both Chroma (semantic search) and BM25 (lexical search).\n",
    "2. Combine the results from both retrievers while deduplicating overlapping results.\n",
    "3. Return the top k most relevant and unique documents.\n",
    "\n",
    "This function plays a vital role in the RAG pipeline by ensuring that the retrieved documents are relevant and diverse, combining semantic understanding with precise keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GYdmjXWFtxca"
   },
   "outputs": [],
   "source": [
    "class EnsembleRetriever:\n",
    "    \"\"\"\n",
    "    Merges results from Chroma similarity search and BM25 lexical search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chroma_store, bm25_retriever):\n",
    "        self.chroma_store = chroma_store\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "    \n",
    "    def get_relevant_documents(self, query: str, k: int = 5):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents by combining results from Chroma and BM25.\n",
    "        \"\"\"\n",
    "        # Get documents from Chroma (semantic search)\n",
    "        chroma_docs = self.chroma_store.similarity_search(query, k=k)\n",
    "        \n",
    "        # Get documents from BM25 (lexical search)\n",
    "        bm25_docs = self.bm25_retriever.retrieve(query, k=k)\n",
    "        \n",
    "        # Convert BM25 results to Document objects if they aren't already\n",
    "        bm25_docs = [\n",
    "            doc if isinstance(doc, Document) else Document(page_content=doc)\n",
    "            for doc in bm25_docs\n",
    "        ]\n",
    "        \n",
    "        # Combine results from both retrievers\n",
    "        combined = chroma_docs + bm25_docs\n",
    "        \n",
    "        # Deduplicate results\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        \n",
    "        for doc in combined:\n",
    "            # Use first 60 chars as deduplication key\n",
    "            content = doc.page_content\n",
    "            key = content[:60]\n",
    "            \n",
    "            if key not in seen:\n",
    "                unique_docs.append(doc)\n",
    "                seen.add(key)\n",
    "                \n",
    "                # Break if we have enough documents\n",
    "                if len(unique_docs) >= k:\n",
    "                    break\n",
    "        \n",
    "        return unique_docs[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86StDSBJwzYj"
   },
   "source": [
    "Run the following code to test the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Yz8G5gThtxZf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Retrieval Results:\n",
      "1. Machine learning automates model building using data.\n",
      "2. Deep learning is a type of machine learning using neural networks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Sample documents\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning automates model building using data.\"),\n",
    "    Document(page_content=\"Deep learning is a type of machine learning using neural networks.\"),\n",
    "    Document(page_content=\"AI includes technologies like machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing focuses on human-computer language interaction.\"),\n",
    "]\n",
    "\n",
    "# Sample Chroma and BM25 retrievers (mock behavior)\n",
    "class MockChroma:\n",
    "    def similarity_search(self, query, k):\n",
    "        return [Document(page_content=\"Machine learning automates model building using data.\")]\n",
    "\n",
    "class MockBM25:\n",
    "    def retrieve(self, query, k):\n",
    "        return [\"Deep learning is a type of machine learning using neural networks.\"]\n",
    "\n",
    "# Initialize mock retrievers\n",
    "chroma = MockChroma()\n",
    "bm25 = MockBM25()\n",
    "\n",
    "# Initialize EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(chroma, bm25)\n",
    "\n",
    "# Test the retriever with a query\n",
    "query = \"What is machine learning?\"\n",
    "results = ensemble_retriever.get_relevant_documents(query, k=3)\n",
    "\n",
    "# Print the results\n",
    "print(\"Ensemble Retrieval Results:\")\n",
    "for idx, doc in enumerate(results, 1):\n",
    "    print(f\"{idx}. {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5atP_1k5wmr9"
   },
   "source": [
    "Ensemble Retrieval Results:\n",
    "1. Machine learning automates model building using data.\n",
    "2. Deep learning is a type of machine learning using neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Lqc0U7qgtwKs"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "class StrOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpqUJMc2yzS-"
   },
   "source": [
    "# 10. Format Documents\n",
    "\n",
    "format_docs(docs):\n",
    "\n",
    "This function takes a list of documents (docs) and formats them into a readable, numbered list. If no documents are provided, it returns a default message indicating the absence of context.\n",
    "\n",
    "style_prompt:\n",
    "\n",
    "This is a prompt template that prepares the input for a neural style transfer task. It asks an AI model to rewrite a given text (original_text) in a specified style, optionally using a contextual snippet (context) from the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3r_YDxMozMZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format a list of documents into a numbered, readable string.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return \"No relevant context found.\"\n",
    "    \n",
    "    # Format each document with a number and cleaned text\n",
    "    snippet_list = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        # Clean the text: remove extra whitespace and normalize spacing\n",
    "        content = ' '.join(doc.page_content.split())\n",
    "        snippet_list.append(f\"{i}. {content}\")\n",
    "    \n",
    "    # Join all formatted snippets with newlines\n",
    "    return '\\n'.join(snippet_list)\n",
    "\n",
    "# Define the style transfer prompt template\n",
    "style_prompt = PromptTemplate(\n",
    "    input_variables=[\"style\", \"context\", \"original_text\"],\n",
    "    template=\"\"\"\n",
    "Use the following context as reference to rewrite the original text in the specified style.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Original Text: {original_text}\n",
    "\n",
    "Please rewrite the text in the following style: {style}\n",
    "\n",
    "Keep the core meaning and factual content but adapt the writing style and tone.\n",
    "Be creative but maintain accuracy and clarity.\n",
    "\n",
    "Rewritten Text:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ58VijEzuPH"
   },
   "source": [
    "Execute the following code to test the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ld8RnXL9zmnw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Documents:\n",
      "\n",
      "1. Machine learning automates data analysis.\n",
      "2. Deep learning uses neural networks to learn patterns.\n",
      "3. Artificial intelligence includes various technologies.\n",
      "\n",
      "Generated Prompt for Style Transfer:\n",
      "\n",
      "\n",
      "Use the following context as reference to rewrite the original text in the specified style.\n",
      "\n",
      "Context:\n",
      "1. Machine learning automates data analysis.\n",
      "2. Deep learning uses neural networks to learn patterns.\n",
      "3. Artificial intelligence includes various technologies.\n",
      "\n",
      "Original Text: Artificial intelligence is transforming the world.\n",
      "\n",
      "Please rewrite the text in the following style: poetic\n",
      "\n",
      "Keep the core meaning and factual content but adapt the writing style and tone.\n",
      "Be creative but maintain accuracy and clarity.\n",
      "\n",
      "Rewritten Text:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/fz5775953fsb5mn1tqdsv6z00000gn/T/ipykernel_99222/3879688757.py:39: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  styled_output = llm(styled_prompt)  # Generate the styled text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Rewritten (Styled) Text ---\n",
      "In the realm of the digital, where silicon hearts beat,\n",
      "Artificial minds weave patterns that mortal minds can't meet.\n",
      "A dance of algorithms, a symphony so sweet,\n",
      "In the world of the artificial, our dreams take flight, so fleet.\n",
      "\n",
      "Machine learning, the analyst of data, so swift and so keen,\n",
      "Deep learning, the weaver of patterns, in the neural networks seen.\n",
      "A tapestry of insights, woven in the deepest machine,\n",
      "In the world of the artificial, knowledge grows like a green scene.\n",
      "\n",
      "Artificial intelligence, the transformer of worlds, so grand,\n",
      "Guiding us through the digital, with a hand held firm and strong,\n",
      "In the world of the artificial, we dance to the beat of a song.\n",
      "\n",
      "A dance of the digital, a waltz through the data,\n",
      "Artificial intelligence, our guide, in the world of the future, so data.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEndpoint  # Or the specific LLM library you're using\n",
    "\n",
    "# Example setup for LLM (ensure this is compatible with your LLM)\n",
    "def setup_llm():\n",
    "    return HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",  # Replace with the appropriate model\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "# Sample documents\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning automates data analysis.\"),\n",
    "    Document(page_content=\"Deep learning uses neural networks to learn patterns.\"),\n",
    "    Document(page_content=\"Artificial intelligence includes various technologies.\"),\n",
    "]\n",
    "\n",
    "# Test the format_docs function\n",
    "formatted_docs = format_docs(sample_docs)\n",
    "print(\"Formatted Documents:\\n\")\n",
    "print(formatted_docs)\n",
    "\n",
    "# Test the style_prompt with sample inputs\n",
    "style = \"poetic\"\n",
    "context = formatted_docs\n",
    "original_text = \"Artificial intelligence is transforming the world.\"\n",
    "\n",
    "styled_prompt = style_prompt.format(\n",
    "    style=style,\n",
    "    context=context,\n",
    "    original_text=original_text,\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Prompt for Style Transfer:\\n\")\n",
    "print(styled_prompt)\n",
    "\n",
    "# Pass the prompt to the LLM\n",
    "llm = setup_llm()  # Initialize the LLM\n",
    "styled_output = llm(styled_prompt)  # Generate the styled text\n",
    "\n",
    "print(\"\\n--- Rewritten (Styled) Text ---\")\n",
    "print(styled_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq3HaHd0CFog"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# 11. RAG chain\n",
    "\n",
    "Use the EnsembleRetriever to retrieve relevant documents from Chroma and BM25.\n",
    "Format the retrieved documents into a readable context.\n",
    "Generate a prompt for neural style transfer using the retrieved context and the input query.\n",
    "Pass the prompt to the LLM and parse the model's response to return the final styled output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nbvH2EZ9zp-d"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def build_rag_chain(llm, chroma_store, bm25_retriever):\n",
    "    \"\"\"\n",
    "    Build a RAG chain using an ensemble retriever with Chroma and BM25.\n",
    "    \"\"\"\n",
    "    # Create the ensemble retriever\n",
    "    ensemble_retriever = EnsembleRetriever(chroma_store, bm25_retriever)\n",
    "    \n",
    "    def retrieve_and_format_context(query, k=5):\n",
    "        \"\"\"Helper function to retrieve and format context\"\"\"\n",
    "        context_docs = ensemble_retriever.get_relevant_documents(query, k=k)\n",
    "        return format_docs(context_docs)\n",
    "    \n",
    "    def rag_chain(inputs):\n",
    "        \"\"\"Process inputs through the RAG pipeline\"\"\"\n",
    "        # Get query and retrieve context\n",
    "        query = inputs[\"question\"]\n",
    "        context = retrieve_and_format_context(query)\n",
    "        \n",
    "        # Generate the prompt using the style template\n",
    "        prompt = style_prompt.format(\n",
    "            style=inputs[\"style\"],\n",
    "            context=context,\n",
    "            original_text=inputs[\"original_text\"]\n",
    "        )\n",
    "        \n",
    "        # Generate styled text using the LLM\n",
    "        llm_output = llm(prompt)\n",
    "        \n",
    "        # Parse the output\n",
    "        parser = StrOutputParser()\n",
    "        result = parser.parse(llm_output)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0NBNIFCDgvE"
   },
   "source": [
    "# 12. Final response\n",
    "\n",
    "1. Scrape content from specified URLs, process the raw text, and split it into smaller, retrievable chunks.\n",
    "2. Build the retrievers: Create a Chroma vector store and a BM25 retriever using the processed documents.\n",
    "3. Build the RAG chain: Set up a pipeline that integrates the retrievers, context formatting, and an LLM to perform neural style transfer.\n",
    "4. Run the application: Accept a user query and a target style, then process the input through the RAG chain to produce styled output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "I1pFwYzOCZvj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Scraping content and splitting into documents...\n",
      "Scraping content from: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "Scraping content from: https://en.wikipedia.org/wiki/Machine_learning\n",
      "Total number of documents: 354\n",
      "Step 2: Building Chroma vector store and BM25 retriever...\n",
      "Step 3: Building RAG chain...\n",
      "\n",
      "Step 4: Neural Style Transfer Demo...\n",
      "\n",
      "============================================\n",
      "        Neural Style Transfer Demo          \n",
      "============================================\n",
      "Original Text : Explain machine learning.\n",
      "Desired Style : as if it were a recipe for cooking\n",
      "\n",
      "Step 5: Running the RAG chain...\n",
      "\n",
      "--- Styled Output ---\n",
      "\n",
      "Title: A Simple Recipe for Machine Learning\n",
      "\n",
      "Ingredients:\n",
      "1. A generous helping of data\n",
      "2. A pinch of statistical analysis\n",
      "3. A dash of mathematical optimization\n",
      "4. A dollop of deep learning (optional but highly recommended)\n",
      "\n",
      "Instructions:\n",
      "1. Gather your data and ensure it's well-prepared. This is the foundation for your dish.\n",
      "2. Sprinkle a pinch of statistical analysis over your data to help you understand its characteristics and patterns.\n",
      "3. Add a dash of mathematical optimization to refine your understanding and make predictions more accurate.\n",
      "4. If you're feeling adventurous, mix in a dollop of deep learning to surpass previous approaches and take your dish to the next level.\n",
      "\n",
      "Serve:\n",
      "Machine learning is a versatile dish that can be applied in various fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. In business, it's known as predictive analytics.\n",
      "\n",
      "History:\n",
      "The term \"machine learning\" was first coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. Since then, many renowned figures, such as logician Walter Pitts and Warren McCulloch, have contributed to the development of machine learning technologies.\n",
      "\n",
      "Variations:\n",
      "- Classification trees: A type of machine learning model where the target variable can take a discrete set of values, represented by leaves, and branches represent conjunctions of features that lead to those class labels.\n",
      "- Regression trees: A type of machine learning model where the target variable can take continuous values, typically real numbers.\n",
      "- Support-vector machines (SVMs): A set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one class, SVMs find the optimal boundary between classes to make predictions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main script for scraping, building retrievers, setting up the RAG chain,\n",
    "    and running a neural style transfer demo.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Scrape content and split into documents\n",
    "    print(\"Step 1: Scraping content and splitting into documents...\")\n",
    "    example_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "        \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "    ]\n",
    "\n",
    "    # Step 1A: Initialize an empty list to store all documents\n",
    "    all_docs = []\n",
    "\n",
    "    # Step 1B: Iterate through the URLs to fetch and process content\n",
    "    for url in example_urls:\n",
    "        print(f\"Scraping content from: {url}\")\n",
    "\n",
    "        # Step 1B.1: Fetch and parse the raw text from the URL\n",
    "        raw_text = fetch_and_parse(url)  # Replace with your implementation\n",
    "\n",
    "        # Step 1B.2: Split the raw text into chunks (documents)\n",
    "        splits = split_text_into_documents(raw_text)  # Replace with your implementation\n",
    "\n",
    "        # Step 1B.3: Add the chunks to the list of documents\n",
    "        all_docs.extend(splits)\n",
    "\n",
    "    print(f\"Total number of documents: {len(all_docs)}\")\n",
    "\n",
    "    # Step 2: Build Chroma and BM25 retrievers\n",
    "    print(\"Step 2: Building Chroma vector store and BM25 retriever...\")\n",
    "\n",
    "    # Step 2A: Build the Chroma vector store\n",
    "    chroma_store = build_chroma(all_docs)  # Replace with your implementation\n",
    "\n",
    "    # Step 2B: Build the BM25 retriever\n",
    "    bm25_retriever = BM25Retriever(all_docs)  # Replace with your implementation\n",
    "\n",
    "    # Step 3: Build the RAG chain\n",
    "    print(\"Step 3: Building RAG chain...\")\n",
    "\n",
    "    # Step 3A: Set up the LLM\n",
    "    llm = setup_llm()  # Replace with your implementation\n",
    "\n",
    "    # Step 3B: Build the RAG chain\n",
    "    rag_chain = build_rag_chain(llm,chroma_store, bm25_retriever)  # Replace with your implementation\n",
    "\n",
    "    # Step 4: Neural Style Transfer Demo\n",
    "    print(\"\\nStep 4: Neural Style Transfer Demo...\")\n",
    "\n",
    "    # Step 4A: Define the user query and target style\n",
    "    user_text = \"Explain machine learning.\"\n",
    "    target_style = \"as if it were a recipe for cooking\"\n",
    "    inputs = {\"question\": user_text, \"style\": target_style, \"original_text\": user_text}\n",
    "\n",
    "    print(\"\\n============================================\")\n",
    "    print(\"        Neural Style Transfer Demo          \")\n",
    "    print(\"============================================\")\n",
    "    print(f\"Original Text : {user_text}\")\n",
    "    print(f\"Desired Style : {target_style}\")\n",
    "\n",
    "    # Step 5: Run the RAG chain\n",
    "    print(\"\\nStep 5: Running the RAG chain...\")\n",
    "\n",
    "    # Hint: Pass `inputs` through the RAG chain to generate styled output.\n",
    "    styled_result = rag_chain(inputs)  # Replace with your implementation\n",
    "\n",
    "    print(\"\\n--- Styled Output ---\")\n",
    "    print(styled_result)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
